{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SID-1706156-Projectcode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate --content-disposition https://github.com/AccentDB/one-speaker-vectors/archive/master.zip\n",
        "!unzip one-speaker-vectors-master.zip"
      ],
      "metadata": {
        "id": "01EM7BZQNGJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import datetime, os, random\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "\n",
        "# Scikit imports for data handling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Keras imports\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense"
      ],
      "metadata": {
        "id": "fXdIaeHLMrwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(v):\n",
        "  # Suppose vector is v:\n",
        "  # convert it to 2 dimensional for intuitive manipulation\n",
        "  x = v.shape[1]\n",
        "  y = v.shape[2]\n",
        "  v = v.reshape((-1, x * y))\n",
        "  # Compute norm\n",
        "  nm = np.linalg.norm(v, axis=1)\n",
        "  # Reshape it so it can be divided\n",
        "  nm = nm.reshape((-1,1))\n",
        "  #  Divide\n",
        "  v = v/nm\n",
        "  # get the matrix\n",
        "  v = v.reshape((-1, x, y))\n",
        "  return v\n",
        "  \n",
        "print('Loading data...')\n",
        "\n",
        "bangla = normalize(np.load(\"one-speaker-vectors-master/bangla_speaker_01.files.npy\"))\n",
        "malayalam = normalize(np.load(\"one-speaker-vectors-master/malayalam_speaker_01.files.npy\"))\n",
        "odiya = normalize(np.load(\"one-speaker-vectors-master/odiya_speaker_01.files.npy\"))\n",
        "telugu = normalize(np.load(\"one-speaker-vectors-master/telugu_speaker_01.files.npy\"))\n",
        "indian = normalize(np.load(\"one-speaker-vectors-master/indian_speaker_01.files.npy\"))\n",
        "australian = normalize(np.load(\"one-speaker-vectors-master/australian_speaker_01.files.npy\"))\n",
        "british = normalize(np.load(\"one-speaker-vectors-master/british_speaker_01.files.npy\"))\n",
        "american = normalize(np.load(\"one-speaker-vectors-master/american_speaker_01.files.npy\"))\n",
        "welsh = normalize(np.load(\"one-speaker-vectors-master/welsh_speaker_01.files.npy\"))\n",
        "\n",
        "print(bangla.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BWFd19wCVyn",
        "outputId": "0ae646cf-edb4-442f-82c4-412f7d23a53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "(778, 499, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categories\n",
        "samples = {\n",
        "  \"bangla\": bangla,\n",
        "  \"malayalam\": malayalam,\n",
        "  \"odiya\": odiya,\n",
        "  \"telugu\": telugu,\n",
        "  \"indian\": indian,\n",
        "  \"australian\": australian,\n",
        "  \"british\": british,\n",
        "  \"american\": american,\n",
        "  \"welsh\": welsh,\n",
        "\n",
        "}\n",
        "\n",
        "indian = [\"bangla\", \"malayalam\", \"odiya\", \"telugu\"]\n",
        "non_indian = [\"australian\", \"british\", \"american\", \"welsh\"]\n",
        "all_accents = indian + non_indian + [\"indian\"]\n",
        "print(\"Indian accents: \", indian)\n",
        "print(\"Non-Indian accents: \", non_indian)\n",
        "print(\"All accents: \", all_accents)"
      ],
      "metadata": {
        "id": "R22YO_L9MwCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accents(accent_category):\n",
        "  \"\"\"\n",
        "      Get the accents corresponding to the category\n",
        "  \"\"\"\n",
        "  accents = None\n",
        "  if category == 'indian':\n",
        "    accents = indian\n",
        "  elif category == 'non_indian':\n",
        "    accents = non_indian\n",
        "  elif category == 'all':\n",
        "    accents = all\n",
        "  \n",
        "  return accents\n",
        "\n",
        "def get_data(accent_category):\n",
        "  \"\"\"\n",
        "      Get the data for the accent category\n",
        "  \"\"\"\n",
        "  accents = get_accents(accent_category)\n",
        "  if accents is None:\n",
        "    return None\n",
        "\n",
        "  x = np.zeros((0, 499, 13))\n",
        "  y = []\n",
        "  label = 0\n",
        "  for accent in accents:\n",
        "    y += (samples[accent].shape[0] * [label])\n",
        "    x = np.concatenate((x, samples[accent]), axis=0)\n",
        "    label += 1\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "2HlcBK_vMzUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constants\n",
        "maxlen = 100\n",
        "nb_filter = 256\n",
        "filter_length_1 = 10\n",
        "filter_length_2 = 5\n",
        "hidden_dims = 750\n",
        "\n",
        "# split ration of 80:20\n",
        "split_ratio = 0.20"
      ],
      "metadata": {
        "id": "l4vi0cKeM1EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_splits(x, y):\n",
        "  \"\"\"\n",
        "      Get train, test and validation splits for the data\n",
        "  \"\"\"\n",
        "  nb_classes = 1+int(np.max(y))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=split_ratio, shuffle=True, random_state=seed)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=split_ratio, shuffle=True, random_state=seed)\n",
        "\n",
        "  Y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
        "  Y_test = keras.utils.to_categorical(y_test, nb_classes)\n",
        "  Y_val = keras.utils.to_categorical(y_val, nb_classes)\n",
        "\n",
        "  print(\"Number of classes : \", nb_classes)\n",
        "\n",
        "  print(\"\\nDATA SPLITS:\")\n",
        "  print(\"Train: \", X_train.shape[0], \"samples\")\n",
        "  print(\"Test: \", X_test.shape[0], \"samples\")\n",
        "  print(\"Val: \", X_val.shape[0], \"samples\\n\")\n",
        "\n",
        "  return X_train, X_test, X_val, Y_train, Y_test, Y_val"
      ],
      "metadata": {
        "id": "mcSp_-xeM4AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 499 dimensions are produced by sampling a 5s audio file at 1ms\n",
        "test_dim = 499\n",
        "# 13 MFCC features \n",
        "input_shape = (test_dim, 13)\n",
        "\n",
        "def build_model(nb_classes):\n",
        "  model = Sequential()\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(32, (3), input_shape=input_shape))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling1D(pool_size=(2)))\n",
        "\n",
        "  model.add(Conv1D(32, (3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling1D(pool_size=(2)))\n",
        "\n",
        "  model.add(Conv1D(64, (3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling1D(pool_size=(2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "test_model = build_model(nb_classes=4)"
      ],
      "metadata": {
        "id": "IVcR6cVdM6XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sizes = [32]\n",
        "nb_epochs = [3]\n",
        "\n",
        "def train(x, y):\n",
        "  \"\"\"\n",
        "      Train the CNN model\n",
        "  \"\"\"\n",
        "  X_train, X_test, X_val, Y_train, Y_test, Y_val = get_data_splits(x, y)\n",
        "\n",
        "  nb_train_samples = X_train.shape\n",
        "  nb_classes = 1+int(np.max(y))\n",
        "\n",
        "  for batch_size in batch_sizes:\n",
        "    for nb_epoch in nb_epochs:\n",
        "      print('Build model...')\n",
        "      print('Batch Size: ', batch_size, 'and Number of epochs: ', nb_epoch ,'\\n')\n",
        "\n",
        "      model = build_model(nb_classes)\n",
        "      model.fit(X_train, Y_train, steps_per_epoch=nb_train_samples[0] // batch_size,\n",
        "              nb_epoch=nb_epoch, shuffle='true', verbose=1, validation_data=(X_val, Y_val),\n",
        "              validation_steps=1, callbacks=[tensorboard_callback])\n",
        "\n",
        "      score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "      print(f\"batch_size: {batch_size} epoch: {nb_epoch}\", score)"
      ],
      "metadata": {
        "id": "lXwTmbTKM9Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvyJ-V8Zu_sv"
      },
      "source": [
        "category = 'indian' # or 'non_indian', or 'all'\n",
        "print(\"Accent category : \", category)\n",
        "\n",
        "(x, y) = get_data(category)\n",
        "print(\"\\nTraining data successfully loaded.\")\n",
        "\n",
        "print(\"Training...\")\n",
        "model = train(x, y)\n",
        "print(\"\\nTraining done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "zWY2hH7pHsNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}